{"_default": {"1": {"idx": "de18eb0d-cc5e-4a53-81a9-efdc2a980a0a", "title": "Text Classification Using Transformers (Pytorch Implementation)", "entity": ["Internet", "Open AI", "Encoder", "Encoder", "Roberta", "Roberta"], "keywords": ["models", "architectures", "output", "outputs", "language model", "network architecture", "new architectural", "transformers", "transforms", "transformation", "transformer", "transforming", "sequence", "layer", "layers", "information", "bert", "text", "min", "networks", "encoder", "encoding", "languages", "decoder", "connected", "connection", "create", "created", "cross", "attention", "times", "time", "notebook", "words", "example", "word based", "previously", "previous", "mentorwritten", "help", "helping", "helpful", "art", "roberta", "health", "uses", "use", "useful", "klai", "data", "fluent", "helmi", "speech", "array", "credits", "neurodata", "multi", "article"], "summary": "That is due to the fact that the information is passed at each step and the longer the chain is, the more probable the information is lost along the chain.i recommend a nice article that talk in depth about the difference between seq2seq and transformer .An image is worth thousand words, so we will start with that!The first thing that we can see is that it has a sequence-to-sequence encoder-decoder architecture.Much of the literature on Transformers present on the Internet use this very architecture to explain Transformers.But this is not the one used in Open AI\u2019s GPT model (or the GPT-2 model, which was just a larger version of its predecessor).The GPT is a 12-layer decoder only transformer with 117M parameters.The Transformer has a stack of 6 Encoder and 6 Decoder, unlike Seq2Seq;the Encoder contains two sub-layers: multi-head self-attention layer and a fully connected feed-forward network.The Decoder contains three sub-layers, a multi-head self-attention layer, an additional layer that performs multi-head self-attention over encoder outputs, and a fully connected feed-forward network.Each sub-layer in Encoder and Decoder has a Residual connection followed by a layer normalization.All input and output tokens to Encoder/Decoder are converted to vectors using learned embeddings.These input embeddings are then passed to Positional Encoding.The Transformers architecture does not contain any recurrence or convolution and hence has no notion of word order.All the words of the input sequence are fed to the networkwith no special order or position as they all flow simultaneously through the Encoder and decoder stack.To understand the meaning of a sentence,it is essential to understand the position and the order of words.It is too simple to use the ClassificationModel from simpletransformes :ClassificationModel(\u2018Architecture\u2019, \u2018model shortcut name\u2019, use_cuda=True,num_labels=4)Architecture : Bert , Roberta , Xlnet , Xlm\u2026shortcut name models for Roberta : roberta-base , roberta-large \u2026.more details herewe create a model that classify text for 4 classes [\u2018art\u2019, \u2018politics\u2019, \u2018health\u2019, \u2018tourism\u2019]we apply this model in our previous projectand we integrate it in our flask application here .", "link": "https://medium.com/swlh/text-classification-using-transformers-pytorch-implementation-5ff9f21bd106"}, "2": {"idx": "7e63f800-c504-49fb-8cee-64e13b47bdcf", "title": "Text Classification Using Transformers (Pytorch Implementation)", "entity": ["Transformers", "Open AI\u2019s", "Decoder", "Decoder", "Decoder", "Encoder/Decoder", "Transformers", "fed", "ClassificationModel", "Bert", "Roberta", "Xlnet", "shortcut", "Roberta", "herewe"], "keywords": ["models", "architectures", "output", "outputs", "language model", "network architecture", "new architectural", "transformers", "transforms", "transformation", "transformer", "transforming", "sequence", "layer", "layers", "information", "bert", "text", "min", "networks", "encoder", "encoding", "languages", "decoder", "connected", "connection", "create", "created", "cross", "attention", "times", "time", "notebook", "words", "example", "word based", "previously", "previous", "mentorwritten", "help", "helping", "helpful", "art", "roberta", "health", "uses", "use", "useful", "klai", "data", "fluent", "helmi", "speech", "array", "credits", "neurodata", "multi", "article"], "summary": "That is due to the fact that the information is passed at each step and the longer the chain is, the more probable the information is lost along the chain.i recommend a nice article that talk in depth about the difference between seq2seq and transformer .An image is worth thousand words, so we will start with that!The first thing that we can see is that it has a sequence-to-sequence encoder-decoder architecture.Much of the literature on Transformers present on the Internet use this very architecture to explain Transformers.But this is not the one used in Open AI\u2019s GPT model (or the GPT-2 model, which was just a larger version of its predecessor).The GPT is a 12-layer decoder only transformer with 117M parameters.The Transformer has a stack of 6 Encoder and 6 Decoder, unlike Seq2Seq;the Encoder contains two sub-layers: multi-head self-attention layer and a fully connected feed-forward network.The Decoder contains three sub-layers, a multi-head self-attention layer, an additional layer that performs multi-head self-attention over encoder outputs, and a fully connected feed-forward network.Each sub-layer in Encoder and Decoder has a Residual connection followed by a layer normalization.All input and output tokens to Encoder/Decoder are converted to vectors using learned embeddings.These input embeddings are then passed to Positional Encoding.The Transformers architecture does not contain any recurrence or convolution and hence has no notion of word order.All the words of the input sequence are fed to the networkwith no special order or position as they all flow simultaneously through the Encoder and decoder stack.To understand the meaning of a sentence,it is essential to understand the position and the order of words.It is too simple to use the ClassificationModel from simpletransformes :ClassificationModel(\u2018Architecture\u2019, \u2018model shortcut name\u2019, use_cuda=True,num_labels=4)Architecture : Bert , Roberta , Xlnet , Xlm\u2026shortcut name models for Roberta : roberta-base , roberta-large \u2026.more details herewe create a model that classify text for 4 classes [\u2018art\u2019, \u2018politics\u2019, \u2018health\u2019, \u2018tourism\u2019]we apply this model in our previous projectand we integrate it in our flask application here .", "link": "https://medium.com/swlh/text-classification-using-transformers-pytorch-implementation-5ff9f21bd106"}}}