The year 2020 has just started but we can already see the trends of Graph Machine Learning (GML) in the latest research papers. Below is my view on what will be important in 2020 for GML and the discussion of these papers.The goal of this article is not on introducing the basic concepts of GML such as graph neural networks (GNNs), but on exposing cutting-edge research that we can see in the top scientific conferences. For the start, I took the submissions to ICLR 2020, one of the most prestigious conferences to submit your work in GML. In the previous post, I already described some quick stats about this domain, but here is a short version of that:There are 150 submissions in GML and every third paper is accepted. This corresponds to approximately 10% of all accepted papers.I read most of those GML papers and here is my list of trends for 2020:Let’s get at each of those trends.I’m particularly excited to see this trend as it shows that the area of GML matures and previous heuristic approaches are being replaced with new theoretical solutions. There is still a lot to understand about graph neural networks, but there were quite a few important results about how GNNs work.I’ll start with my favorite: What graph neural networks cannot learn: depth vs width by Andreas Loukas. This paper is a striking balance between technical simplicity, high practical impact, and far-reaching theoretical insights.It shows that the dimension of the node embeddings (width of the network, w) multiplied by the number of layers (depth of the network, d) should be proportional to the size of the graph n, i.e. dw = O(n), if we want GNN being able to compute a solution to popular graph problems (like cycle detection, diameter estimation, vertex cover, etc.).As a consequence, many current implementations of GNN fail to achieve this condition as the number of layers (~2–5 in many implementations) and the dimension of embeddings (~100–1000) is not big enough compared to the size of the graph. On the other hand, bigger networks are too prohibitive in the current setting, raising a question about how we should design efficient GNNs, — something that we need to resolve in the future. Quite eloquently, the paper also draws inspiration from the distributed computational models from the ’80s to show that GNNs essentially do the same thing. There are more results inside so I recommend taking a look at it.In a similar vein, two other papers, Oono & Suzuki and Barceló et al., study the power of GNNs. The first one, Graph Neural Networks Exponentially Lose Expressive Power for Node Classification, shows that:Under certain conditions on the weights, GCNs cannot learn anything except node degrees and connected components (determined by the spectra of the Laplacian) when the number of layers grows.This result is a generalization of the well-known property that Markov processes converge to the unique equilibrium, where the rates of the convergence are determined by the eigenvalues of the transition matrix.In the second paper, The Logical Expressiveness of Graph Neural Networks, the authors show a connection between GNNs and the types of node classifiers they can capture. We already know that some GNNs are as powerful as WL test of isomorphism, i.e. two nodes are colored the same by WL if and only if they are classified the same by GNNs. But can other classification functions be captured by GNN? For example, imagine a boolean function that assigns true to all nodes if and only if a graph has an isolated vertex. Can GNNs capture this logic? Intuitively no, because GNN is a message-passing mechanism and if there is no link between one part of the graph and the other (two connected components), there will be no message passed between the two. So a proposed simple fix is to add a readout operation after neighborhood aggregation so that now each node has information about all other nodes in the graph when it updates all features.Other works on theory include measuring the use of graph information for GNN by Hou et al. and the equivalence of role-based and distance-based node embeddings by Srinivasan & Ribeiro.It’s also great to see how GNNs can be applied for real-world tasks. This year there are applications in fixing bugs in Javascript, game playing, answering IQ-like tests, optimization of TensorFlow computational graphs, molecule generation, and question generation in dialogue systems.In HOPPITY: Learning Graph Transformations to Detect and Fix Bugs in Programs Dinella et al. present a way to detect and fix bugs simultaneously in Javascript code. The code is transformed into an abstract syntax tree, which is then preprocessed by GNN to obtain a code embedding. The idea is given a graph in its initial state to modify it through multiple rounds of graph edit operators (addition or deletion of nodes, replacing a node value or type). To understand which nodes of the graph should be modified they employ a Pointer network that takes a graph embedding and the history of edits so far and chooses the node. Then, a fix is performed using the LSTM network that also takes the graph embedding and the context of the edit. The authors verified the approach on the commits from GitHub, showing significant boost to other, less general baselines. Similar in spirit the work by Wei et al. LambdaNet: Probabilistic Type Inference using Graph Neural Networks studies how to infer the types of variables for languages like Python or TypeScript. The authors present a type dependency hypergraph, which contains the variables of the program as nodes and the relationships between them such as logical (e.g. boolean type) or contextual (e.g. similar variable name) constraints. Then a GNN model is first trained to produce embeddings for variables of the graphs and the possible types, which are then used to predict the type with the highest likelihood. In the experiments, LambdaNet performs higher both in standard variable types (e.g. boolean) and user-defined types.A paper by Wang et al. Abstract Diagrammatic Reasoning with Multiplex Graph Networks shows how to reason in IQ-like tests (Raven Progressive Matrices (RPM) and Diagram Syllogism (DS)) with GNNs. In the RPM task, a graph is composed for each row of the matrix, for which the edge embeddings are obtained by a feedforward model, followed by a graph summarization. Since there are 8 possible answers for the last row, 8 different graphs are created and each is concatenated with the first two rows to get a prediction of the IQ score by a ResNet model.A DeepMind’s paper Reinforced Genetic Algorithm Learning for Optimizing Computation Graphs proposes an RL algorithm to optimize the cost of TensorFlow computation graphs. The graphs are processed via a standard message-passing GNN, which produces discretized embeddings which correspond to scheduling priorities of each node in the graph. These embeddings are fed into a genetic algorithm BRKGA, which decides on the device placement and scheduling of each node. The model is trained to optimize the real computational cost of the obtained TensorFlow graph.Other interesting applications of GNN include molecule generation by Shi et al., game playing by Jiang et al., and dialogue systems by Chen et al.There are quite a few papers this year on reasoning in knowledge graphs. Essentially, a knowledge graph is a structured way to represent facts. Unlike general graphs, in knowledge graph nodes and edges actually bear some meaning such as the name of the actor or act of playing in movies (see image below). A common problem on knowledge graph is answering complex queries such as “Which movies by Steven Spielberg won Oscars before 2000?”, which can be translated into a logical query ∨ {Win(Oscar, V) ∧ Directed(Spielberg, V) ∧ ProducedBefore(2000, V) }.In the paper Query2box: Reasoning over Knowledge Graphs in Vector Space Using Box Embeddings Ren et al. proposes to embed a query into a latent space not as a single point, but as a rectangular box. This approach allows performing natural intersection operation, i.e. conjunction ∧, as it results in the new rectangular box. However, modeling a union, i.e. disjunction ∨, is not that straightforward, as it may result in non-overlapping regions. Moreover, to model accurately any query with embeddings, the complexity of the distance function between embeddings measured by the VC dimension should be proportional to the number of entities in the graph. Instead, there is a nice trick to replace a disjunctive query into a DNF form, where a union happens only at the end of the computation graph, which effectively reduces to simple distance computation to each of the subqueries.In a similar topic, Wang et al. propose a way to work with numerical entities and rules in the paper titled “Differentiable Learning of Numerical Rules in Knowledge Graphs”. For example, for a citation knowledge graph, you can have a rule influences(Y,X) ← colleagueOf(Z,Y) ∧ supervisorOf(Z,X)∧ hasCitation>(Y,Z) , which states that typically students X are influenced by a colleague Y of their supervisor Z, who has more citations. Every relation in the right-hand side of this rule can be represented as a matrix and the process of finding missing links can be formulated as a consecutive matrix multiplication of relations by the entity vector, — a process called Rule Learning. Neural approaches can work only with categorical rules such as colleagueOf(Z,Y) due to the way the matrices are constructed. The authors’ contribution is on a novel way to work efficiently with numerical rules such as hasCitation>(Y,Z) and negation operators by showing that in reality there is no need to materialize such matrices explicitly, which significantly reduces the running time.Another theme appearing more frequently in machine learning in general and this year in GML is the re-evaluation of the existing models and how do they perform in a fair environment. As such a paper You CAN Teach an Old Dog New Tricks! On Training Knowledge Graph Embeddings by Ruffinelli et al. shows that the performance of new models often depends on the “minor” details of the experimental training such as the form of the loss function, regularizer, and sampling schemes. In a large ablation study, the authors observe that old methods such as the RESCAL model can achieve SOTA performance just by properly tuning hyperparameters.There are many other interesting works in this domain. Allen et al. build on recent insights in word embeddings to understand more about the latent space of the learned representations of relations and entities. Asai et al. show how a model can retrieve a reasoning path over the Wikipedia graph that answers a given query. Tabacof & Costabello touches on an important topic of probability calibration of graph embedding models. They show that popular embedding models TransE and ComplEx that obtain probabilities by transforming logits with sigmoid function are poorly calibrated, i.e. either under-predict or over-predict the existence of the fact. Their method relies on generating the corrupted triples as negatives that are used by known approaches such as Platt scaling and isotonic regression to calibrate the probabilities.Graph embeddings have been a long topic for graph machine learning and this year there are new perspectives on how we should approach learning graph representations.Deng et al. present a way to improve running time and accuracy in node classification problem for any unsupervised embedding method in the work GraphZoom: A Multi-level Spectral Approach for Accurate and Scalable Graph Embedding. The overall idea is to first reduce the original graph to a much smaller graph, for which you can compute node embeddings quickly, and then recover the embeddings of the original graph. In the beginning, the original graph is augmented with additional edges that correspond to the links between k-nearest neighbors of nodes based on attribute similarity. Then, the graph is coarsened: each node is projected into a lower-dimensional space via local spectral methods and aggregated into clusters. Any unsupervised graph embedding method such as DeepWalk or Deep Graph Infomax can be used to get node embeddings on the small graph. In the final step, the obtained node embeddings, which essentially represent embeddings of the clusters are iteratively broadcasted back with a smoothing operator to prevent different nodes having the same embedding. In experiments, the GraphZoom framework achieves astounding 40x speedup over node2vec and DeepWalk methods with a 10% improvement of accuracy.Several papers have scrutinized the previous results of graph classification problem. A Fair Comparison of Graph Neural Networks for Graph Classification by Errica et al. contributed on the fair re-evaluation of GNN models on this problem, showing that a simple baseline that does not utilize a topology of the graph (i.e. it works on the aggregated node features) performs on par with the SOTA GNNs. This surprising phenomenon apparently was published before by Orlova et al. in 2015 but didn’t reach a big audience. A nice outcome of this work is fair SOTA results on popular data sets and convenient code base in PyTorch-Geometric to run a comparison of the models for future papers. In our work Understanding Isomorphism Bias in Graph Data Sets, we also discover that in commonly used data sets such as MUTAG or IMDB a lot of graphs have isomorphic copies, even if one considers the node attributes. Moreover, among these isomorphic graphs, many have different target labels, which naturally introduces label noise for the classifier. This indicates the importance of usage of all available meta-information of the network such as node or edge attributes for better performance of the model. Another work Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification by Chen et al. shows that if one replaces a non-linear neighborhood aggregation function with its linear counterpart, which includes degrees of the neighbors and the propagated graph attributes, then the performance of the model does not decrease, — this is aligned with previous statements that many graph data sets are trivial for classification and raises a question of the proper validation framework for this task.With the growing rate of submissions in the top conferences, we can anticipate many interesting results in the field of GML in 2020. We can already see a transformation of this field from heuristic applications of deep learning on graphs to more sound approaches and fundamental questions about the scope of graph models. GNNs found its place as an effective solution to many practical problems that can be formulated with graphs, but I would expect that in general GML has just scratched the surface of what we can achieve in the intersection of graph theory and machine learning and we should stay tuned to the upcoming results.P.S. I will continue to write about graph machine learning, so if you are interested, follow me on medium or subscribe to my telegram channel or my twitter.Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a lookWritten byWritten by In previous chapter, we discussed how different methodologies attempted to classify each pixel in an image. In this article, we will elaborate how instance segmentation classifies each pixel and identifies unique occurrences within a category.In summer 2014, based on R-CNN, Simultaneous Detection and Segmentation (SDS) was conceived. As seen on the image below, it consists of 4 phases.For Proposal Generation, Multiscale Combinatorial Grouping (MCG) was used to generate 2000 region candidates per image.From the input image a multiresolution image pyramid was created. Afterwards, hierarchical segmentation for each scale is performed. By aligning these multiple hierarchies and combining them into a single multiscale segmentation hierarchy, we can group those components and produce a list of object proposals.For each region, a CNN is used for Feature Extraction. For both cropped boxes and region foregrounds individual CNN's were trained, consisting of 5 convolutions followed by 2 fully connected layers. Those features are then fed into a linear SVM, i.e. Region Classification, to assign a score for each category to each candidate. Regions with a score above a threshold are finally fed into the Region Refinement module.For each region, we first learn to predict a coarse, top-down figure-ground mask which is then discretized. Using the features from the CNN and the discretized figure-ground mask, a logistic regression classifier is trained to predict the probability that a cell belongs to the foreground. This results in a prediction about the shape of the object but does not necessarily respect the bottom-up contours. As such, a second classifier is trained to combine this coarse mask with the region candidate. For each pixel we then predict if it belongs to the original region candidate.September 2015, Facebook AI Research (FAIR) team proposed the first algorithm to learn to generate segmentation proposals directly from raw image data, DeepMask. Based on a Convolutional Networks, for a given input image, it will generate a class-agnostic mask and associated score estimating the likelihood of a patch containing a centered object.Using transfer learning on a pretrained VGG for classification on ImageNet, the architecture above was retrained on MS COCO. As such, a large part of the network is shared for the aforementioned tasks. Removing the fully connected layer, we then create a brand for each task. The top branch represents the segmentation head whereas the bottom branch is the objectness branch.The segmentation head is constructed of a 1x1 convolution followed by a classification layer consisting of h x w pixel classifier. Each classifier is responsible to assign whether the corresponding pixel belongs to the object. To achieve this, each pixelwise classifier must utilize the entire feature map so that the network will output a mask for each single object present in an image.The objectness head predicts if an image patch satisfies the following:The output is a score indicating the presence of an object satisfying the previously mentioned constraints.The FAIR team introduces in 2016 an improvement over DeepMask. Then, the encoder-decoder architecture proved to be a feasible solution to generate pixelwise fine-grained segmentations. Similar to what was discussed in previous story, by combining information from the low-level features which comes from early layers with high-level object information coming from deeper layers in the network, the coarseness issue is addressed.Utilising an improved DeepMask architecture for the forward pass, a coarse mask encoding is generated, representing a feature map with multiple channels. This is then progressively integrated with information from previous layers by the use of refinement modules. Each refinement module doubles the spatial resolution by combining the feature map from the forward pass with the mask generated from the previous refinement module until full resolution is achieved and a final output encodes the object mask.Departing from Fast-RCNN, FAIR addresses the following detection challenges :Once more departing from DeepMask, MultiPathNet introduces 3 major modifications:Each input image is fed into the VGG where ROI pooling is used to extract features from the region proposals. For each object proposal, 4 different region crops are created and centered on the object proposal for the purpose of viewing the object at multiple scales, so-called foveal regions. Each region crop is passed through a fully connected layers, concatenated and passed to a classification and regression head.If you recall, Fast R-CNN performs RoI-pooling after the 5th layer in VGG. As such, features have been downsampled by a factor of 16 and thus most spatial information will have been lost for smaller objects. Effective localisation of small objects requires higher-resolution features from earlier layers and thus we take the features from the 3rd to 5th convolution layers and provide them to each foveal classifier.Typically, during training a fixed IoU threshold is chosen. In this case, an integral loss function was introduced to encourage the model to perform well at multiple IoU thresholds.We will start with arguably the most popular architecture created by the Facebook AI Research team. As we will see, Faster R-CNN constitutes the fundamental block on which Mask R-CNN is built upon. As such, let us refresh some concepts.Recall that Faster R-CNN generates 2 outputs: class labels and boundary boxes. After extracting features, a region proposal network (RPN) is employed to generate region of interests (RoI). These RoI are then resized and passed to a set of fully convolutional (FC) layers.At its essence, Mask R-CNN extends Faster R-CNN with a 3rd branch which generates the output masks. By overlaying on each bounding box a mask and taking into account the class label, we would obtain a semantic segmentation result. Additionally, each bounding box delineates a specific instance of an object resulting in segmentation masks which denotes the various instances of a class.The training is formulated as a multi task loss which is the sum of the classification loss, bounding-box loss and mask loss.A nuanced but big contribution of Mask RCNN is the introduction of ROI Align. If normal pooling were to be used we would basically introduce rounding errors twice in the pipeline, as seen below.The first error occurs when coordinates from the original input image are mapped to the coordinates on the feature map. The second error, is at the pooling phase itself where those rounded off coordinates get quantized and results into a loss of information.ROI Align solves this issue with the following steps:This modification can result in an increase of up to 10% making Mask RCNN a reference in the instance segmentation literature and a widely used architecture for a broad set of applications.In 2018, researchers propose PANet by improving the propagation in Mask RCNN. The 3 contributions are:Features are extracted using a FPN backbone. Based on the fact that neurons in the higher layers respond to objects whereas neurons in the lower level responds more likely to local patterns and structures and that low-level patterns and structures are a high indicator of instances. We can conclude that increased instance localisation can occur if propagating strong responses of low-level patterns. This propagation, illustrated above with the dashed green line, is created by introducing a shortcut of less than 10 layers instead of the 100+ layers.The bottom-up path augmentation happens on the output of the FPN which results in feature maps (Ni) having the same spatial size as the corresponding FPN output stage (Pi).To reduce the spatial size, feature map Ni first goes through a 3×3 conv layer with stride 2 and ReLU.Finally, featuremap Ni+1 is created by fusing both maps and passing it through another 3x3 conv layer and ReLU.Each proposal is then mapped to different feature levels and fed into Mask R-CNN's ROIAlign layer where feature grids from each level is pooled. The feature grids are then fused using a sum or elementwise-max.The motivation follows the one stated earlier. High-level features have a large receptive field and capture richer context information, combining them with small proposals can be beneficial due to the fact that low-level features contain many fine details and high localisation accuracy.Similar to Mask R-CNN, the output of the pooling feeds 3 branches to compute the class, box and masks. A fully connected layer takes care of class classification and box regression. Below, we observe the pipeline for the mask component.Each conv layer consists of 256 3×3 filters and the deconvolutional layer up-samples feature with factor 2. Again similar to Mask R-CNN, segmentation and classification are decoupled by predicting a binary pixel-wise mask for each class independently. From conv3 a small branch is created where a fully connected layer predicts a class-agnostic foreground/background mask. Doing this, increases the generality and reduces the loss in spatial information.Fall 2019, the first YOLO-based model was created by breaking instance segmentation into two parallel tasks: generating a set of prototype masks and predicting per-instance mask coefficient. Combining those prototype masks with the mask coefficient results in instance masks. Typical instance segmentation approaches will first generate candidate region of interests followed by classification and segmentation. YOLACT differs in this matter by adding a mask branch to an existing one-stage object detection model without any feature localization steps.Based on a RetinaNet backbone, two branches are created. Firstly, Protonet, which is a FCN, generates image-sized prototype masks independent of instance. Secondly, the Prediction Head predicts a vector of mask coefficients corresponding to each prototype.Pixels of the same instance have a higher likelihood of being from the same instance. Convolutional layers do take this into account whereas a fully connected layer doesn’t. As such, we split the branches where fully-connected layers do produce excellent semantic vectors and let the convolution layers create spatially coherent masks.Protonet is a simple sequence of 3x3 convolutions except for the final convolution which is 1x1. The deepest layer of FPN is used to produce higher resolution prototypes resulting in both higher quality masks and better performance on smaller objects.Anchor-based object detectors have two branches in their prediction heads: one to calculate class confidence and one to calculate the bounding box regressor. Turns out by simply adding a branch, mask coefficients for each prototype can be calculated."YOLACT learns how to localize instances on its own via different activations in its prototypes."One would then assume that increasing the amount of prototypes would increase the performance. Predicting the mask coefficients is not easy and thus, if coefficients are too erroneous due the linear combination of prototype masks and coefficients the resulting instance mask can completely vanish or include other objects. As such, a balancing act is required to obtain the right amount of prototypes and corresponding coefficients.Introduces some cool upgrades to YOLACT:Using a ResNet as backbone, each 3x3 convolutional layer was replaced by a deformable convolution layer which improves the handling of instances with different scales, aspect ratios and rotations.As we know by now, standard convolution operates on a grid of an input with a predefined size. To accommodate geometric variations we can either build a dataset with sufficient variations or employ transformation-invariant features and algorithms. As we see below, by adding a learnable offset to a regular grid a freedom exist to generalize across various transformations, aspect ratios and rotations.Prediction heads were optimized by simply tuning the hyper-parameters of the anchor-based backbone. Variations were made by "keeping the scales unchanged while increasing the anchor aspect ratios and keeping the aspect ratios unchanged while increasing the scales per FPN level by threefold."It was observed that mask segmentations with high quality don’t necessarily have higher class confidences. By learning to regress the predicted mask to its mask IoU with ground-truth, the network learns to better correlate class confidence with mask quality. As seen, it consists of 6 convolutional layers (followed by ReLU) and 1 global pooling layer.It is clear to understand the challenges present in the task of instance segmentation. The recurring question which all architectures are trying to answer is related to how to the global visual context of the image take into account to improve the prediction of the segmentation. Instance segmentation being the crème de la crème, as compared to image classification, object detection and semantic segmentation, is a hot topic. Fine in granularity it also provides a first glance in obtaining contextual information. Many architectures are focused in improving performances regardless (more or less) of computational intensity, it can be expected to see an increase in architectures developed toward real-timeness and mobile applications.Written byWritten by
In previous chapter, we discussed how different methodologies attempted to classify each pixel in an image. In this article, we will elaborate how instance segmentation classifies each pixel and identifies unique occurrences within a category.In summer 2014, based on R-CNN, Simultaneous Detection and Segmentation (SDS) was conceived. As seen on the image below, it consists of 4 phases.For Proposal Generation, Multiscale Combinatorial Grouping (MCG) was used to generate 2000 region candidates per image.From the input image a multiresolution image pyramid was created. Afterwards, hierarchical segmentation for each scale is performed. By aligning these multiple hierarchies and combining them into a single multiscale segmentation hierarchy, we can group those components and produce a list of object proposals.For each region, a CNN is used for Feature Extraction. For both cropped boxes and region foregrounds individual CNN's were trained, consisting of 5 convolutions followed by 2 fully connected layers. Those features are then fed into a linear SVM, i.e. Region Classification, to assign a score for each category to each candidate. Regions with a score above a threshold are finally fed into the Region Refinement module.For each region, we first learn to predict a coarse, top-down figure-ground mask which is then discretized. Using the features from the CNN and the discretized figure-ground mask, a logistic regression classifier is trained to predict the probability that a cell belongs to the foreground. This results in a prediction about the shape of the object but does not necessarily respect the bottom-up contours. As such, a second classifier is trained to combine this coarse mask with the region candidate. For each pixel we then predict if it belongs to the original region candidate.September 2015, Facebook AI Research (FAIR) team proposed the first algorithm to learn to generate segmentation proposals directly from raw image data, DeepMask. Based on a Convolutional Networks, for a given input image, it will generate a class-agnostic mask and associated score estimating the likelihood of a patch containing a centered object.Using transfer learning on a pretrained VGG for classification on ImageNet, the architecture above was retrained on MS COCO. As such, a large part of the network is shared for the aforementioned tasks. Removing the fully connected layer, we then create a brand for each task. The top branch represents the segmentation head whereas the bottom branch is the objectness branch.The segmentation head is constructed of a 1x1 convolution followed by a classification layer consisting of h x w pixel classifier. Each classifier is responsible to assign whether the corresponding pixel belongs to the object. To achieve this, each pixelwise classifier must utilize the entire feature map so that the network will output a mask for each single object present in an image.The objectness head predicts if an image patch satisfies the following:The output is a score indicating the presence of an object satisfying the previously mentioned constraints.The FAIR team introduces in 2016 an improvement over DeepMask. Then, the encoder-decoder architecture proved to be a feasible solution to generate pixelwise fine-grained segmentations. Similar to what was discussed in previous story, by combining information from the low-level features which comes from early layers with high-level object information coming from deeper layers in the network, the coarseness issue is addressed.Utilising an improved DeepMask architecture for the forward pass, a coarse mask encoding is generated, representing a feature map with multiple channels. This is then progressively integrated with information from previous layers by the use of refinement modules. Each refinement module doubles the spatial resolution by combining the feature map from the forward pass with the mask generated from the previous refinement module until full resolution is achieved and a final output encodes the object mask.Departing from Fast-RCNN, FAIR addresses the following detection challenges :Once more departing from DeepMask, MultiPathNet introduces 3 major modifications:Each input image is fed into the VGG where ROI pooling is used to extract features from the region proposals. For each object proposal, 4 different region crops are created and centered on the object proposal for the purpose of viewing the object at multiple scales, so-called foveal regions. Each region crop is passed through a fully connected layers, concatenated and passed to a classification and regression head.If you recall, Fast R-CNN performs RoI-pooling after the 5th layer in VGG. As such, features have been downsampled by a factor of 16 and thus most spatial information will have been lost for smaller objects. Effective localisation of small objects requires higher-resolution features from earlier layers and thus we take the features from the 3rd to 5th convolution layers and provide them to each foveal classifier.Typically, during training a fixed IoU threshold is chosen. In this case, an integral loss function was introduced to encourage the model to perform well at multiple IoU thresholds.We will start with arguably the most popular architecture created by the Facebook AI Research team. As we will see, Faster R-CNN constitutes the fundamental block on which Mask R-CNN is built upon. As such, let us refresh some concepts.Recall that Faster R-CNN generates 2 outputs: class labels and boundary boxes. After extracting features, a region proposal network (RPN) is employed to generate region of interests (RoI). These RoI are then resized and passed to a set of fully convolutional (FC) layers.At its essence, Mask R-CNN extends Faster R-CNN with a 3rd branch which generates the output masks. By overlaying on each bounding box a mask and taking into account the class label, we would obtain a semantic segmentation result. Additionally, each bounding box delineates a specific instance of an object resulting in segmentation masks which denotes the various instances of a class.The training is formulated as a multi task loss which is the sum of the classification loss, bounding-box loss and mask loss.A nuanced but big contribution of Mask RCNN is the introduction of ROI Align. If normal pooling were to be used we would basically introduce rounding errors twice in the pipeline, as seen below.The first error occurs when coordinates from the original input image are mapped to the coordinates on the feature map. The second error, is at the pooling phase itself where those rounded off coordinates get quantized and results into a loss of information.ROI Align solves this issue with the following steps:This modification can result in an increase of up to 10% making Mask RCNN a reference in the instance segmentation literature and a widely used architecture for a broad set of applications.In 2018, researchers propose PANet by improving the propagation in Mask RCNN. The 3 contributions are:Features are extracted using a FPN backbone. Based on the fact that neurons in the higher layers respond to objects whereas neurons in the lower level responds more likely to local patterns and structures and that low-level patterns and structures are a high indicator of instances. We can conclude that increased instance localisation can occur if propagating strong responses of low-level patterns. This propagation, illustrated above with the dashed green line, is created by introducing a shortcut of less than 10 layers instead of the 100+ layers.The bottom-up path augmentation happens on the output of the FPN which results in feature maps (Ni) having the same spatial size as the corresponding FPN output stage (Pi).To reduce the spatial size, feature map Ni first goes through a 3×3 conv layer with stride 2 and ReLU.Finally, featuremap Ni+1 is created by fusing both maps and passing it through another 3x3 conv layer and ReLU.Each proposal is then mapped to different feature levels and fed into Mask R-CNN's ROIAlign layer where feature grids from each level is pooled. The feature grids are then fused using a sum or elementwise-max.The motivation follows the one stated earlier. High-level features have a large receptive field and capture richer context information, combining them with small proposals can be beneficial due to the fact that low-level features contain many fine details and high localisation accuracy.Similar to Mask R-CNN, the output of the pooling feeds 3 branches to compute the class, box and masks. A fully connected layer takes care of class classification and box regression. Below, we observe the pipeline for the mask component.Each conv layer consists of 256 3×3 filters and the deconvolutional layer up-samples feature with factor 2. Again similar to Mask R-CNN, segmentation and classification are decoupled by predicting a binary pixel-wise mask for each class independently. From conv3 a small branch is created where a fully connected layer predicts a class-agnostic foreground/background mask. Doing this, increases the generality and reduces the loss in spatial information.Fall 2019, the first YOLO-based model was created by breaking instance segmentation into two parallel tasks: generating a set of prototype masks and predicting per-instance mask coefficient. Combining those prototype masks with the mask coefficient results in instance masks. Typical instance segmentation approaches will first generate candidate region of interests followed by classification and segmentation. YOLACT differs in this matter by adding a mask branch to an existing one-stage object detection model without any feature localization steps.Based on a RetinaNet backbone, two branches are created. Firstly, Protonet, which is a FCN, generates image-sized prototype masks independent of instance. Secondly, the Prediction Head predicts a vector of mask coefficients corresponding to each prototype.Pixels of the same instance have a higher likelihood of being from the same instance. Convolutional layers do take this into account whereas a fully connected layer doesn’t. As such, we split the branches where fully-connected layers do produce excellent semantic vectors and let the convolution layers create spatially coherent masks.Protonet is a simple sequence of 3x3 convolutions except for the final convolution which is 1x1. The deepest layer of FPN is used to produce higher resolution prototypes resulting in both higher quality masks and better performance on smaller objects.Anchor-based object detectors have two branches in their prediction heads: one to calculate class confidence and one to calculate the bounding box regressor. Turns out by simply adding a branch, mask coefficients for each prototype can be calculated."YOLACT learns how to localize instances on its own via different activations in its prototypes."One would then assume that increasing the amount of prototypes would increase the performance. Predicting the mask coefficients is not easy and thus, if coefficients are too erroneous due the linear combination of prototype masks and coefficients the resulting instance mask can completely vanish or include other objects. As such, a balancing act is required to obtain the right amount of prototypes and corresponding coefficients.Introduces some cool upgrades to YOLACT:Using a ResNet as backbone, each 3x3 convolutional layer was replaced by a deformable convolution layer which improves the handling of instances with different scales, aspect ratios and rotations.As we know by now, standard convolution operates on a grid of an input with a predefined size. To accommodate geometric variations we can either build a dataset with sufficient variations or employ transformation-invariant features and algorithms. As we see below, by adding a learnable offset to a regular grid a freedom exist to generalize across various transformations, aspect ratios and rotations.Prediction heads were optimized by simply tuning the hyper-parameters of the anchor-based backbone. Variations were made by "keeping the scales unchanged while increasing the anchor aspect ratios and keeping the aspect ratios unchanged while increasing the scales per FPN level by threefold."It was observed that mask segmentations with high quality don’t necessarily have higher class confidences. By learning to regress the predicted mask to its mask IoU with ground-truth, the network learns to better correlate class confidence with mask quality. As seen, it consists of 6 convolutional layers (followed by ReLU) and 1 global pooling layer.It is clear to understand the challenges present in the task of instance segmentation. The recurring question which all architectures are trying to answer is related to how to the global visual context of the image take into account to improve the prediction of the segmentation. Instance segmentation being the crème de la crème, as compared to image classification, object detection and semantic segmentation, is a hot topic. Fine in granularity it also provides a first glance in obtaining contextual information. Many architectures are focused in improving performances regardless (more or less) of computational intensity, it can be expected to see an increase in architectures developed toward real-timeness and mobile applications.Written byWritten by